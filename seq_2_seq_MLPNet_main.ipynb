{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3892de",
   "metadata": {},
   "source": [
    "#### package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8089c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchinwei/anaconda3/envs/mambastock/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Callable, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mamba import Mamba, MambaConfig\n",
    "from loss_functions import get_loss_fn\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90293414",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fa9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '0730_2330/'\n",
    "use_cuda = True\n",
    "n_steps=10\n",
    "window=30\n",
    "patience = 100\n",
    "val_ratio = 0.2\n",
    "epochs = 1000\n",
    "loss_fcn = \"mse\"\n",
    "\n",
    "lr = 0.0005\n",
    "wd = 1e-5\n",
    "hidden = 32\n",
    "layer = 3\n",
    "n_test = 350\n",
    "ts_code = 2330\n",
    "risk_free = 0.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4616fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_return_features(data, price_col='close', windows=[5, 10, 20]):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = data.copy()\n",
    "    returns = df[price_col].pct_change()\n",
    "\n",
    "    # 基本報酬率\n",
    "    df['returns'] = returns\n",
    "\n",
    "    for window in windows:\n",
    "        # 移動平均報酬率\n",
    "        df[f'returns_ma_{window}'] = returns.rolling(window=window).mean()\n",
    "        \n",
    "        # 指數移動平均\n",
    "        df[f'returns_ema_{window}'] = returns.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "        # 報酬率波動度\n",
    "        df[f'returns_volatility_{window}'] = returns.rolling(window=window).std()\n",
    "\n",
    "        # 報酬率的 Z-score\n",
    "        mean = returns.rolling(window=window).mean()\n",
    "        std = returns.rolling(window=window).std()\n",
    "        df[f'returns_zscore_{window}'] = (returns - mean) / std\n",
    "\n",
    "    return df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ac912",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5320500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(str(ts_code)+\"_value\"+'.csv')\n",
    "data['trade_date'] = pd.to_datetime(data['trade_date'], format='%Y/%m/%d')\n",
    "\n",
    "# 加入星期幾（0=星期一, 6=星期日）\n",
    "data['day_of_week'] = data['trade_date'].dt.dayofweek\n",
    "\n",
    "# 加入月份（1～12）\n",
    "data['month'] = data['trade_date'].dt.month\n",
    "\n",
    "returns = data['close_TW_roc'] \n",
    "\n",
    "tp=[5, 10, 20]\n",
    "\n",
    "for tp in tp:\n",
    "    # 移動平均報酬率\n",
    "    data[f'returns_ma_{tp}'] = returns.rolling(window=tp).mean()\n",
    "    \n",
    "    # 指數移動平均\n",
    "    data[f'returns_ema_{tp}'] = returns.ewm(span=tp, adjust=False).mean()\n",
    "\n",
    "    # 報酬率波動度\n",
    "    data[f'returns_volatility_{tp}'] = returns.rolling(window=tp).std()\n",
    "\n",
    "    # 報酬率的 Z-score\n",
    "    mean = returns.rolling(window=tp).mean()\n",
    "    std = returns.rolling(window=tp).std()\n",
    "    data[f'returns_zscore_{tp}'] = (returns - mean) / std\n",
    "\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "['trade_date', 'stock_no', 'stock_name', 'stock_type', \n",
    "'open_TW', 'close_TW', close_TW_roc, 'h_TW', 'l_TW', 'vol_TW', 'CBOE_SKEW_INDEX', \n",
    "'CBOE_Volatility_INDEX', 'COPPER_F', 'COPPER', 'GOLD_F', \n",
    "'GOLD', 'OIL_F', 'OIL', 'SP_F', 'SILVER_F', 'SILVER', \n",
    "'TWII', 'IXIC', 'GSPC', 'DJI', 'NYSE', 'RUSSELL', 'SSE', \n",
    "'FCHI', 'FTSE', 'GDAXI', 'Nikkei_F', 'IXIC_F', 'DJI_F', \n",
    "'S_P_F', 'USDX_F', 'JPY', 'GBP', 'HKD', 'CNY', 'AUD', \n",
    "'TWD', 'EUR', 'DTB4WK', 'DTB3', 'DTB6', 'DGS5', 'DGS1', \n",
    "'DAAA', 'DBAA', 'DGS3MO', 'DGS6MO', 'DGS1.1', 'DCOILWTICO', \n",
    "'TE1', 'TE2', 'TE3', 'TE5', 'TE6', 'DE1', 'DE2', 'DE4', 'DE5', \n",
    "'DE6', 'DCOILWTICO_rel_change', 'day_of_week', 'month']\n",
    "'''\n",
    "\n",
    "index = ['open_TW', 'close_TW', 'h_TW', 'l_TW', 'vol_TW', 'CBOE_SKEW_INDEX', \n",
    "'CBOE_Volatility_INDEX', 'TWII', 'DJI', 'NYSE', 'TWD', 'CNY', 'day_of_week', 'month',\n",
    "\"returns_ma_5\", \"returns_ema_5\", \"returns_volatility_5\",\"returns_zscore_5\",\n",
    "\"returns_ma_10\", \"returns_ema_10\", \"returns_volatility_10\", \"returns_zscore_10\",\n",
    "\"returns_ma_20\", \"returns_ema_20\", \"returns_volatility_20\", \"returns_zscore_20\"]\n",
    "\n",
    "# 拆離 label 欄位\n",
    "\n",
    "ratechg = data['close_TW_roc'].values\n",
    "\n",
    "features = data[index].values\n",
    "num_samples = features.shape[0]\n",
    "# num_samples: 2520\n",
    "\n",
    "# 定義 horizon 數\n",
    "horizon = n_steps\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for i in range(num_samples - window - horizon + 1):\n",
    "    X_win = features[i : i + window]                        # shape: [window, features]\n",
    "    t = np.arange(window).reshape(-1, 1)  # shape: [window, 1]\n",
    "    sin_t = np.sin(t / 10000)\n",
    "    cos_t = np.cos(t / 10000)\n",
    "    time_encoding = np.concatenate([sin_t, cos_t], axis=1)    # shape: [window, 2]\n",
    "\n",
    "    X_win = np.concatenate([X_win, time_encoding], axis=1) \n",
    "    y_future = ratechg[i + window : i + window + horizon]     # shape: [horizon]\n",
    "    X_list.append(X_win)\n",
    "    y_list.append(y_future)\n",
    "\n",
    "trainX = np.array(X_list[:-n_test])\n",
    "trainy = np.array(y_list[:-n_test])\n",
    "testX = np.array(X_list[-n_test:])\n",
    "testy = np.array(y_list[-n_test:])\n",
    "\n",
    "# trainX.shape: (2146, 15, 61)  2146 = num_samples - window - horizon + 1\n",
    "# trainy.shape: (2146, 10)\n",
    "# testX.shape: (350, 15, 61)\n",
    "# testy.shape: (350, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc2b81",
   "metadata": {},
   "source": [
    "#### train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1ed3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(len(trainX) * val_ratio)\n",
    "\n",
    "valX = trainX[-val_size:]\n",
    "valy = trainy[-val_size:]\n",
    "\n",
    "trainX = trainX[:-val_size]\n",
    "trainy = trainy[:-val_size]\n",
    "\n",
    "# trainX.shape: (1717, 15, 61)\n",
    "# trainy.shape: (1717, 10)\n",
    "# valX.shape: (429, 15, 61)\n",
    "# valy.shape: (429, 10)\n",
    "\n",
    "in_dim = trainX.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98053803",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_tensor = torch.tensor(trainX, dtype=torch.float32)\n",
    "trainy_tensor = torch.tensor(trainy, dtype=torch.float32)\n",
    "\n",
    "valX_tensor = torch.tensor(valX, dtype=torch.float32)\n",
    "valy_tensor = torch.tensor(valy, dtype=torch.float32)\n",
    "\n",
    "testX_tensor = torch.tensor(testX, dtype=torch.float32)\n",
    "testy_tensor = torch.tensor(testy, dtype=torch.float32)\n",
    "\n",
    "# Build datasets and loaders\n",
    "train_dataset = TensorDataset(trainX_tensor, trainy_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(valX_tensor, valy_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(testX_tensor, testy_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76877df",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e507a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    多層 Mamba 時序模型，用於序列輸入特徵的未來預測。\n",
    "\n",
    "    參數:\n",
    "        in_dim (int): features 數量\n",
    "        horizon (int): 模型預測的未來時間步數（輸出維度）。\n",
    "        hidden (int): 中間表示層（latent layer）的維度。\n",
    "        layer (int): Mamba block 的堆疊層數。\n",
    "\n",
    "    屬性:\n",
    "        config (MambaConfig): 用於初始化 Mamba 模型的配置。\n",
    "        proj_in (nn.Linear): 將輸入特徵從 in_dim 投影到 hidden 維度。\n",
    "        mamba (Mamba): 多層 Mamba 模型，用於處理時間序列資料。\n",
    "        pool (nn.AdaptiveAvgPool1d): 算平均\n",
    "        head (nn.Linear): 轉換為 horizon 維度的預測向量。\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    horizon: int\n",
    "    hidden: int\n",
    "    layer: int\n",
    "\n",
    "    config: MambaConfig\n",
    "    proj_in: nn.Linear\n",
    "    mamba: Mamba\n",
    "    pool: nn.AdaptiveAvgPool1d\n",
    "    head: nn.Linear\n",
    "\n",
    "    def __init__(self, in_dim: int, horizon: int, hidden: int, layer: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.horizon = horizon\n",
    "        self.hidden = hidden\n",
    "        self.layer = layer\n",
    "\n",
    "        self.config = MambaConfig(d_model=hidden, n_layers=layer)\n",
    "        self.proj_in = nn.Linear(in_dim, hidden)\n",
    "        self.mamba = Mamba(self.config)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        x = self.proj_in(x)            # x: [B, window, in_dim] → [B, window, hidden]\n",
    "        x = self.mamba(x)              #                        → [B, window, hidden]\n",
    "        x = x.transpose(1, 2)          #                        → [B, hidden, window]\n",
    "        x = self.pool(x).squeeze(-1)   #                        → [B, hidden]\n",
    "        return self.head(x)            #                        → [B, horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1031b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected MLP 模型，用於序列特徵的未來預測。\n",
    "\n",
    "    每個時間步的特徵會被展平成一個長向量，送進 MLP 層中做預測。\n",
    "\n",
    "    輸入:\n",
    "        x: [B, window, in_dim] → 展平 → [B, window * in_dim]\n",
    "\n",
    "    輸出:\n",
    "        預測向量 [B, horizon]\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    horizon: int\n",
    "    hidden: int\n",
    "    window: int\n",
    "    flatten_dim: int\n",
    "    fc: nn.Sequential\n",
    "    \n",
    "    def __init__(self, in_dim: int, horizon: int, hidden: int, window: int) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten_dim: int = window * in_dim\n",
    "        self.fc: nn.Sequential = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)  # [B, window * in_dim]\n",
    "        return self.fc(x)          # → [B, horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc3dad",
   "metadata": {},
   "source": [
    "#### dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d027d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataBatch:\n",
    "    train_loader: DataLoader\n",
    "    val_loader: DataLoader\n",
    "    test_loader: DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e3ea5",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8901e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson(pred, target):\n",
    "    pred_np = pred.detach().cpu().view(-1).numpy().astype(np.float64)\n",
    "    target_np = target.detach().cpu().view(-1).numpy().astype(np.float64)\n",
    "    return pearsonr(pred_np, target_np)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739802fa",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf8627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    device: str \n",
    "    model: torch.nn.Module\n",
    "    loss_fn: str\n",
    "    opt: torch.optim.Optimizer\n",
    "    train_losses: list\n",
    "    val_losses: list\n",
    "    \n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, loss_fn: str, opt: torch.optim.Optimizer, scheduler, device: str = \"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = get_loss_fn(loss_fn)\n",
    "        self.opt = opt\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        databatch: DataBatch,\n",
    "        epochs: int,\n",
    "        verbose: bool = True,\n",
    "        filename: Optional[str] = None,\n",
    "        patience: int = 10\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        counter = 0\n",
    "        \n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        best_epoch = -1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "\n",
    "            for X_batch, y_batch in databatch.train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "\n",
    "                pred = self.model(X_batch)\n",
    "                loss = self.loss_fn(pred, y_batch)\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss_total = 0.0\n",
    "                for valX, valy in databatch.val_loader:\n",
    "                    valX = valX.to(self.device)\n",
    "                    valy = valy.to(self.device)\n",
    "                    val_pred = self.model(valX)\n",
    "                    val_loss_total += self.loss_fn(val_pred, valy).item()\n",
    "                val_loss_avg = val_loss_total / len(databatch.val_loader)\n",
    "\n",
    "            self.train_losses.append(epoch_train_loss / len(databatch.train_loader))\n",
    "            self.val_losses.append(val_loss_avg)\n",
    "            self.scheduler.step(val_loss_avg)\n",
    "            \n",
    "\n",
    "            current_lr = self.opt.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch}: lr={current_lr:.6f}, ValLoss={val_loss_avg:.8f}\")\n",
    "\n",
    "            if val_loss_avg < best_val_loss:\n",
    "                best_val_loss = val_loss_avg\n",
    "                best_model_state = self.model.state_dict()\n",
    "                best_epoch = epoch\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"[Epoch {epoch+1}] Train: {epoch_train_loss / len(databatch.train_loader):.4f} | Val: {val_loss_avg:.4f}\")\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        if filename:\n",
    "            path = f\"{filename}_{timestamp}.pth\"\n",
    "        else:\n",
    "            path = f\"checkpoint_{timestamp}.pth\"       \n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "        \"model_state\": best_model_state,\n",
    "        \"epoch\": best_epoch,\n",
    "        \"val_loss\": best_val_loss\n",
    "        }, path)\n",
    "        \n",
    "        print(f\"Best checkpoint saved at epoch {best_epoch+1}, Val Loss: {best_val_loss:.4f}\")\n",
    "        self.model.load_state_dict(best_model_state)\n",
    "    \n",
    "        preds, targets, test_loss, r2 = self.evaluate(databatch)\n",
    "        pearson_corr = compute_pearson(preds, targets)\n",
    "\n",
    "        log_entry = (\n",
    "            f\"{path}\\n\"\n",
    "            f\"R²: {r2:.4f} | Pearson: {pearson_corr:.4f}\\n\"\n",
    "            f\"{'-'*35}\\n\"\n",
    "        )\n",
    "\n",
    "        with open(\"training_log.txt\", \"a\") as f:\n",
    "            f.write(log_entry)\n",
    "        \n",
    "        return self.train_losses, self.val_losses\n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        databatch: DataBatch,\n",
    "    ) -> Tuple[torch.Tensor, float]:\n",
    "\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for testX, testy in databatch.test_loader:\n",
    "                testX = testX.to(self.device)\n",
    "                testy = testy.to(self.device)\n",
    "\n",
    "                pred = self.model(testX)\n",
    "                loss = self.loss_fn(pred, testy)\n",
    "\n",
    "                all_preds.append(pred.cpu())\n",
    "                all_targets.append(testy.cpu())\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        preds = torch.cat(all_preds)\n",
    "        targets = torch.cat(all_targets)\n",
    "        r2 = r2_score(targets, preds)\n",
    "        avg_loss = total_loss / len(databatch.test_loader)\n",
    "\n",
    "        print(f\"Test Loss: {avg_loss:.4f} | R² Score: {r2:.4f}\")\n",
    "        return preds, targets, avg_loss, r2\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"val_losses\": self.val_losses\n",
    "        }, path)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1a501",
   "metadata": {},
   "source": [
    "#### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfbd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: lr=0.000500, ValLoss=1291874269.62962961\n",
      "Epoch 1: lr=0.000500, ValLoss=75180595.83123033\n",
      "Epoch 2: lr=0.000500, ValLoss=17026341.63089907\n",
      "Epoch 3: lr=0.000500, ValLoss=8042344.50724615\n",
      "Epoch 4: lr=0.000500, ValLoss=5290818.06700839\n",
      "Epoch 5: lr=0.000500, ValLoss=3852492.54478225\n",
      "Epoch 6: lr=0.000500, ValLoss=2928564.29775609\n",
      "Epoch 7: lr=0.000500, ValLoss=2693235.11984682\n",
      "Epoch 8: lr=0.000500, ValLoss=2289242.95545348\n",
      "Epoch 9: lr=0.000500, ValLoss=2133528.19620107\n",
      "Epoch 10: lr=0.000500, ValLoss=2069721.75828965\n",
      "Epoch 11: lr=0.000500, ValLoss=1905233.55038046\n",
      "Epoch 12: lr=0.000500, ValLoss=1849439.11751565\n",
      "Epoch 13: lr=0.000500, ValLoss=1832920.23050029\n",
      "Epoch 14: lr=0.000500, ValLoss=1770132.06413443\n",
      "Epoch 15: lr=0.000500, ValLoss=1713671.80734502\n",
      "Epoch 16: lr=0.000500, ValLoss=1643309.94044991\n",
      "Epoch 17: lr=0.000500, ValLoss=1617343.36638000\n",
      "Epoch 18: lr=0.000500, ValLoss=1545020.81835101\n",
      "Epoch 19: lr=0.000500, ValLoss=1493086.81430286\n",
      "Epoch 20: lr=0.000500, ValLoss=1447151.22026595\n",
      "Epoch 21: lr=0.000500, ValLoss=1420320.50267481\n",
      "Epoch 22: lr=0.000500, ValLoss=1396802.07110917\n",
      "Epoch 23: lr=0.000500, ValLoss=1360171.69126409\n",
      "Epoch 24: lr=0.000500, ValLoss=1344913.37699309\n",
      "Epoch 25: lr=0.000500, ValLoss=1331874.00629822\n",
      "Epoch 26: lr=0.000500, ValLoss=1315920.22571807\n",
      "Epoch 27: lr=0.000500, ValLoss=1319353.71696553\n",
      "Epoch 28: lr=0.000500, ValLoss=1311287.65969913\n",
      "Epoch 29: lr=0.000500, ValLoss=1306108.78932896\n",
      "Epoch 30: lr=0.000500, ValLoss=1292213.63655142\n",
      "Epoch 31: lr=0.000500, ValLoss=1299753.87266268\n",
      "Epoch 32: lr=0.000500, ValLoss=1291812.29858853\n",
      "Epoch 33: lr=0.000500, ValLoss=1284715.27544032\n",
      "Epoch 34: lr=0.000500, ValLoss=1285454.99303274\n",
      "Epoch 35: lr=0.000500, ValLoss=1285454.99303252\n",
      "Epoch 36: lr=0.000500, ValLoss=1285198.84488411\n",
      "Epoch 37: lr=0.000500, ValLoss=1284888.62266158\n",
      "Epoch 38: lr=0.000500, ValLoss=1284888.62266127\n",
      "Epoch 39: lr=0.000500, ValLoss=1284888.62266092\n",
      "Epoch 40: lr=0.000500, ValLoss=1290949.36340131\n",
      "Epoch 41: lr=0.000500, ValLoss=1290949.36340090\n",
      "Epoch 42: lr=0.000500, ValLoss=1284815.25228935\n",
      "Epoch 43: lr=0.000500, ValLoss=1284815.25228890\n",
      "Epoch 44: lr=0.000250, ValLoss=1284815.25228842\n",
      "Epoch 45: lr=0.000250, ValLoss=1284815.25228817\n",
      "Epoch 46: lr=0.000250, ValLoss=1284815.25228790\n",
      "Epoch 47: lr=0.000250, ValLoss=1293653.62265801\n",
      "Epoch 48: lr=0.000250, ValLoss=1284815.25228733\n",
      "Epoch 49: lr=0.000250, ValLoss=1284815.25228702\n",
      "Epoch 50: lr=0.000250, ValLoss=1284815.25228669\n",
      "Epoch 51: lr=0.000250, ValLoss=1284815.25228633\n",
      "Epoch 52: lr=0.000250, ValLoss=1284815.25228597\n",
      "Epoch 53: lr=0.000250, ValLoss=1284815.25228556\n",
      "Epoch 54: lr=0.000250, ValLoss=1284815.25228514\n",
      "Epoch 55: lr=0.000125, ValLoss=1284815.25228471\n",
      "Epoch 56: lr=0.000125, ValLoss=1284815.25228448\n",
      "Epoch 57: lr=0.000125, ValLoss=1284815.25228423\n",
      "Epoch 58: lr=0.000125, ValLoss=1284815.26154324\n",
      "Epoch 59: lr=0.000125, ValLoss=1284815.26154297\n",
      "Epoch 60: lr=0.000125, ValLoss=1284815.26154271\n",
      "Epoch 61: lr=0.000125, ValLoss=1284815.26154241\n",
      "Epoch 62: lr=0.000125, ValLoss=1284815.26154209\n",
      "Epoch 63: lr=0.000125, ValLoss=1284815.26154175\n",
      "Epoch 64: lr=0.000125, ValLoss=1284815.26154140\n",
      "Epoch 65: lr=0.000125, ValLoss=1284815.26154103\n",
      "Epoch 66: lr=0.000063, ValLoss=1284815.26154062\n",
      "Epoch 67: lr=0.000063, ValLoss=1284815.26154042\n",
      "Epoch 68: lr=0.000063, ValLoss=1284815.26154023\n",
      "Epoch 69: lr=0.000063, ValLoss=1284815.26154000\n",
      "Epoch 70: lr=0.000063, ValLoss=1284815.26153975\n",
      "Epoch 71: lr=0.000063, ValLoss=1284815.26153949\n",
      "Epoch 72: lr=0.000063, ValLoss=1284815.26153923\n",
      "Epoch 73: lr=0.000063, ValLoss=1284815.26153897\n",
      "Epoch 74: lr=0.000063, ValLoss=1284815.26153867\n",
      "Epoch 75: lr=0.000063, ValLoss=1284815.26153836\n",
      "Epoch 76: lr=0.000063, ValLoss=1284815.26153802\n",
      "Epoch 77: lr=0.000031, ValLoss=1284815.26153766\n",
      "Epoch 78: lr=0.000031, ValLoss=1284815.26153746\n",
      "Epoch 79: lr=0.000031, ValLoss=1284815.26153727\n",
      "Epoch 80: lr=0.000031, ValLoss=1284815.26153707\n",
      "Epoch 81: lr=0.000031, ValLoss=1284815.26153687\n",
      "Epoch 82: lr=0.000031, ValLoss=1284815.26153664\n",
      "Epoch 83: lr=0.000031, ValLoss=1284815.26153640\n",
      "Epoch 84: lr=0.000031, ValLoss=1284815.26153614\n",
      "Epoch 85: lr=0.000031, ValLoss=1284815.26153588\n",
      "Epoch 86: lr=0.000031, ValLoss=1284815.26153562\n",
      "Epoch 87: lr=0.000031, ValLoss=1284815.26153532\n",
      "Epoch 88: lr=0.000016, ValLoss=1284815.26153501\n",
      "Epoch 89: lr=0.000016, ValLoss=1284815.26153481\n",
      "Epoch 90: lr=0.000016, ValLoss=1284815.26153461\n",
      "Epoch 91: lr=0.000016, ValLoss=1284815.26153441\n",
      "Epoch 92: lr=0.000016, ValLoss=1284815.26153422\n",
      "Epoch 93: lr=0.000016, ValLoss=1284815.26153402\n",
      "Epoch 94: lr=0.000016, ValLoss=1284815.26153382\n",
      "Epoch 95: lr=0.000016, ValLoss=1284815.26153360\n",
      "Epoch 96: lr=0.000016, ValLoss=1284815.26153335\n",
      "Epoch 97: lr=0.000016, ValLoss=1284815.26153310\n",
      "Epoch 98: lr=0.000016, ValLoss=1284815.26153284\n",
      "Epoch 99: lr=0.000008, ValLoss=1284815.26153258\n",
      "Epoch 100: lr=0.000008, ValLoss=1284815.26153239\n",
      "Epoch 101: lr=0.000008, ValLoss=1284815.26153220\n",
      "Epoch 102: lr=0.000008, ValLoss=1284815.26153200\n",
      "Epoch 103: lr=0.000008, ValLoss=1284815.26153180\n",
      "Early stopping triggered at epoch 104\n",
      "Best checkpoint saved at epoch 34, Val Loss: 1284715.2754\n",
      "Test Loss: 6501112.5131 | R² Score: -12692905245.4993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([27336223242.867924,\n",
       "  542794374.3018868,\n",
       "  32306416.558614347,\n",
       "  9780162.119131872,\n",
       "  4796267.097777266,\n",
       "  2590767.350424816,\n",
       "  1577468.758153325,\n",
       "  1035391.4672524652,\n",
       "  656542.8242775084,\n",
       "  476448.3782150565,\n",
       "  350058.9305677286,\n",
       "  263602.073922364,\n",
       "  198508.0716747824,\n",
       "  152725.93960515704,\n",
       "  113628.86996499084,\n",
       "  86502.73074561758,\n",
       "  65148.80925257115,\n",
       "  47882.3547680298,\n",
       "  36091.71274284168,\n",
       "  26221.9836883158,\n",
       "  18523.80971902607,\n",
       "  12887.02402339641,\n",
       "  8752.25947002383,\n",
       "  5915.384533520744,\n",
       "  3773.4507146163774,\n",
       "  2493.6114925796887,\n",
       "  1537.58375426793,\n",
       "  957.8817176623446,\n",
       "  517.0612205041239,\n",
       "  305.5920445606253,\n",
       "  176.92007781330602,\n",
       "  107.3901375917419,\n",
       "  47.49237262645631,\n",
       "  31.622108533758812,\n",
       "  10.059498873389906,\n",
       "  8.329819689838672,\n",
       "  3.5324897474416024,\n",
       "  0.8174467553771189,\n",
       "  2.114436158550165,\n",
       "  0.49140034807529653,\n",
       "  0.3765192384620742,\n",
       "  0.04097924334049787,\n",
       "  0.03376361773402061,\n",
       "  0.025943739963519685,\n",
       "  0.024336323818578472,\n",
       "  0.021680377871852438,\n",
       "  0.01929516778415385,\n",
       "  0.017476812681569805,\n",
       "  0.12142342233257193,\n",
       "  0.017472789095679543,\n",
       "  0.015406409647526606,\n",
       "  0.015405484598200276,\n",
       "  0.015402305871248245,\n",
       "  0.015402934698493412,\n",
       "  0.015403186753040777,\n",
       "  0.015403591307266703,\n",
       "  0.015402643800765838,\n",
       "  0.015402924998680938,\n",
       "  0.0130304476281382,\n",
       "  0.013025499230145284,\n",
       "  0.013026616975383938,\n",
       "  0.013027471806502567,\n",
       "  0.013027576035556366,\n",
       "  0.013027479467948652,\n",
       "  0.013027129369735156,\n",
       "  0.013025526519653932,\n",
       "  0.013024768836023111,\n",
       "  0.01302406241027814,\n",
       "  0.013024506053694014,\n",
       "  0.01302514220851491,\n",
       "  0.013024393802965587,\n",
       "  0.013025769621204093,\n",
       "  0.013024081467246672,\n",
       "  0.013024183693078329,\n",
       "  0.013021915386659358,\n",
       "  0.013023345660910292,\n",
       "  0.013021749866036873,\n",
       "  0.013021379269940674,\n",
       "  0.013022486032601798,\n",
       "  0.013025281783896234,\n",
       "  0.01302187209773176,\n",
       "  0.013019937301441183,\n",
       "  0.013021051461966533,\n",
       "  0.013019229882871205,\n",
       "  0.013021329751695102,\n",
       "  0.013020389063178369,\n",
       "  0.01301899678864569,\n",
       "  0.013018948096289949,\n",
       "  0.013019655145845323,\n",
       "  0.013017858914538936,\n",
       "  0.013018605826457716,\n",
       "  0.013015776073103244,\n",
       "  0.013018823017910967,\n",
       "  0.013018777418249059,\n",
       "  0.013016226077628023,\n",
       "  0.013017250470957666,\n",
       "  0.013018351733543963,\n",
       "  0.013018751508152147,\n",
       "  0.013016409381240044,\n",
       "  0.013016831560306391,\n",
       "  0.01301810029402094,\n",
       "  0.013014773829435965,\n",
       "  0.013015917186045422,\n",
       "  0.013016141582069532],\n",
       " [1291874269.6296296,\n",
       "  75180595.83123033,\n",
       "  17026341.630899068,\n",
       "  8042344.507246153,\n",
       "  5290818.067008387,\n",
       "  3852492.544782245,\n",
       "  2928564.2977560917,\n",
       "  2693235.1198468176,\n",
       "  2289242.955453476,\n",
       "  2133528.196201066,\n",
       "  2069721.7582896545,\n",
       "  1905233.5503804644,\n",
       "  1849439.1175156473,\n",
       "  1832920.2305002925,\n",
       "  1770132.0641344313,\n",
       "  1713671.8073450222,\n",
       "  1643309.9404499056,\n",
       "  1617343.36638,\n",
       "  1545020.8183510082,\n",
       "  1493086.814302856,\n",
       "  1447151.2202659545,\n",
       "  1420320.502674811,\n",
       "  1396802.0711091715,\n",
       "  1360171.6912640915,\n",
       "  1344913.3769930939,\n",
       "  1331874.0062982219,\n",
       "  1315920.225718068,\n",
       "  1319353.716965528,\n",
       "  1311287.659699126,\n",
       "  1306108.7893289556,\n",
       "  1292213.636551419,\n",
       "  1299753.8726626844,\n",
       "  1291812.2985885267,\n",
       "  1284715.2754403176,\n",
       "  1285454.9930327383,\n",
       "  1285454.9930325246,\n",
       "  1285198.8448841092,\n",
       "  1284888.6226615822,\n",
       "  1284888.6226612732,\n",
       "  1284888.622660925,\n",
       "  1290949.3634013098,\n",
       "  1290949.3634008972,\n",
       "  1284815.2522893513,\n",
       "  1284815.2522888975,\n",
       "  1284815.2522884237,\n",
       "  1284815.252288167,\n",
       "  1284815.252287904,\n",
       "  1293653.622658013,\n",
       "  1284815.2522873343,\n",
       "  1284815.2522870237,\n",
       "  1284815.2522866894,\n",
       "  1284815.2522863338,\n",
       "  1284815.2522859662,\n",
       "  1284815.2522855585,\n",
       "  1284815.252285138,\n",
       "  1284815.2522847091,\n",
       "  1284815.2522844793,\n",
       "  1284815.2522842337,\n",
       "  1284815.2615432357,\n",
       "  1284815.2615429733,\n",
       "  1284815.261542711,\n",
       "  1284815.2615424057,\n",
       "  1284815.2615420937,\n",
       "  1284815.261541752,\n",
       "  1284815.2615413952,\n",
       "  1284815.2615410313,\n",
       "  1284815.2615406225,\n",
       "  1284815.2615404245,\n",
       "  1284815.2615402276,\n",
       "  1284815.2615399954,\n",
       "  1284815.2615397486,\n",
       "  1284815.2615394895,\n",
       "  1284815.2615392278,\n",
       "  1284815.2615389656,\n",
       "  1284815.2615386678,\n",
       "  1284815.2615383568,\n",
       "  1284815.2615380185,\n",
       "  1284815.2615376622,\n",
       "  1284815.2615374643,\n",
       "  1284815.2615372667,\n",
       "  1284815.261537069,\n",
       "  1284815.2615368718,\n",
       "  1284815.2615366434,\n",
       "  1284815.2615363973,\n",
       "  1284815.2615361407,\n",
       "  1284815.2615358785,\n",
       "  1284815.2615356161,\n",
       "  1284815.2615353188,\n",
       "  1284815.2615350077,\n",
       "  1284815.26153481,\n",
       "  1284815.261534612,\n",
       "  1284815.2615344147,\n",
       "  1284815.2615342173,\n",
       "  1284815.2615340198,\n",
       "  1284815.261533822,\n",
       "  1284815.2615335998,\n",
       "  1284815.2615333532,\n",
       "  1284815.2615330985,\n",
       "  1284815.2615328368,\n",
       "  1284815.2615325751,\n",
       "  1284815.2615323926,\n",
       "  1284815.2615321958,\n",
       "  1284815.261531998,\n",
       "  1284815.2615318])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = DataBatch(\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    ")\n",
    "\n",
    "regressor = MLPNet(in_dim=in_dim, horizon=horizon, hidden=hidden, window=window)\n",
    "opt = torch.optim.Adam(regressor.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=10)\n",
    "trainer = Trainer(model=regressor, loss_fn=\"mse\", opt=opt, scheduler=scheduler, device=\"cuda\")\n",
    "trainer.fit(databatch=databatch, epochs=epochs, verbose=False, filename=\"0730_2330/2330\", patience=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec205adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambastock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3892de",
   "metadata": {},
   "source": [
    "#### package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8089c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchinwei/anaconda3/envs/mambastock/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Callable, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mamba import Mamba, MambaConfig\n",
    "from loss_functions import get_loss_fn\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90293414",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fa9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '0730_2330/'\n",
    "use_cuda = True\n",
    "n_steps=10\n",
    "window=30\n",
    "patience = 100\n",
    "val_ratio = 0.2\n",
    "epochs = 1000\n",
    "loss_fcn = \"mse\"\n",
    "\n",
    "lr = 0.0005\n",
    "wd = 1e-5\n",
    "hidden = 32\n",
    "layer = 3\n",
    "n_test = 350\n",
    "ts_code = 2330\n",
    "risk_free = 0.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4616fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_return_features(data, price_col='close', windows=[5, 10, 20]):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = data.copy()\n",
    "    returns = df[price_col].pct_change()\n",
    "\n",
    "    # 基本報酬率\n",
    "    df['returns'] = returns\n",
    "\n",
    "    for window in windows:\n",
    "        # 移動平均報酬率\n",
    "        df[f'returns_ma_{window}'] = returns.rolling(window=window).mean()\n",
    "        \n",
    "        # 指數移動平均\n",
    "        df[f'returns_ema_{window}'] = returns.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "        # 報酬率波動度\n",
    "        df[f'returns_volatility_{window}'] = returns.rolling(window=window).std()\n",
    "\n",
    "        # 報酬率的 Z-score\n",
    "        mean = returns.rolling(window=window).mean()\n",
    "        std = returns.rolling(window=window).std()\n",
    "        df[f'returns_zscore_{window}'] = (returns - mean) / std\n",
    "\n",
    "    return df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ac912",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5320500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(str(ts_code)+\"_value\"+'.csv')\n",
    "data['trade_date'] = pd.to_datetime(data['trade_date'], format='%Y/%m/%d')\n",
    "\n",
    "# 加入星期幾（0=星期一, 6=星期日）\n",
    "data['day_of_week'] = data['trade_date'].dt.dayofweek\n",
    "\n",
    "# 加入月份（1～12）\n",
    "data['month'] = data['trade_date'].dt.month\n",
    "\n",
    "returns = data['close_TW_roc'] \n",
    "\n",
    "tp=[5, 10, 20]\n",
    "\n",
    "for tp in tp:\n",
    "    # 移動平均報酬率\n",
    "    data[f'returns_ma_{tp}'] = returns.rolling(window=tp).mean()\n",
    "    \n",
    "    # 指數移動平均\n",
    "    data[f'returns_ema_{tp}'] = returns.ewm(span=tp, adjust=False).mean()\n",
    "\n",
    "    # 報酬率波動度\n",
    "    data[f'returns_volatility_{tp}'] = returns.rolling(window=tp).std()\n",
    "\n",
    "    # 報酬率的 Z-score\n",
    "    mean = returns.rolling(window=tp).mean()\n",
    "    std = returns.rolling(window=tp).std()\n",
    "    data[f'returns_zscore_{tp}'] = (returns - mean) / std\n",
    "\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "['trade_date', 'stock_no', 'stock_name', 'stock_type', \n",
    "'open_TW', 'close_TW', close_TW_roc, 'h_TW', 'l_TW', 'vol_TW', 'CBOE_SKEW_INDEX', \n",
    "'CBOE_Volatility_INDEX', 'COPPER_F', 'COPPER', 'GOLD_F', \n",
    "'GOLD', 'OIL_F', 'OIL', 'SP_F', 'SILVER_F', 'SILVER', \n",
    "'TWII', 'IXIC', 'GSPC', 'DJI', 'NYSE', 'RUSSELL', 'SSE', \n",
    "'FCHI', 'FTSE', 'GDAXI', 'Nikkei_F', 'IXIC_F', 'DJI_F', \n",
    "'S_P_F', 'USDX_F', 'JPY', 'GBP', 'HKD', 'CNY', 'AUD', \n",
    "'TWD', 'EUR', 'DTB4WK', 'DTB3', 'DTB6', 'DGS5', 'DGS1', \n",
    "'DAAA', 'DBAA', 'DGS3MO', 'DGS6MO', 'DGS1.1', 'DCOILWTICO', \n",
    "'TE1', 'TE2', 'TE3', 'TE5', 'TE6', 'DE1', 'DE2', 'DE4', 'DE5', \n",
    "'DE6', 'DCOILWTICO_rel_change', 'day_of_week', 'month']\n",
    "'''\n",
    "\n",
    "index = ['open_TW', 'close_TW', 'h_TW', 'l_TW', 'vol_TW', 'CBOE_SKEW_INDEX', \n",
    "'CBOE_Volatility_INDEX', 'TWII', 'DJI', 'NYSE', 'TWD', 'CNY', 'day_of_week', 'month',\n",
    "\"returns_ma_5\", \"returns_ema_5\", \"returns_volatility_5\",\"returns_zscore_5\",\n",
    "\"returns_ma_10\", \"returns_ema_10\", \"returns_volatility_10\", \"returns_zscore_10\",\n",
    "\"returns_ma_20\", \"returns_ema_20\", \"returns_volatility_20\", \"returns_zscore_20\"]\n",
    "\n",
    "# 拆離 label 欄位\n",
    "\n",
    "ratechg = data['close_TW_roc'].values\n",
    "\n",
    "features = data[index].values\n",
    "num_samples = features.shape[0]\n",
    "# num_samples: 2520\n",
    "\n",
    "# 定義 horizon 數\n",
    "horizon = n_steps\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for i in range(num_samples - window - horizon + 1):\n",
    "    X_win = features[i : i + window]                        # shape: [window, features]\n",
    "    t = np.arange(window).reshape(-1, 1)  # shape: [window, 1]\n",
    "    sin_t = np.sin(t / 10000)\n",
    "    cos_t = np.cos(t / 10000)\n",
    "    time_encoding = np.concatenate([sin_t, cos_t], axis=1)    # shape: [window, 2]\n",
    "\n",
    "    X_win = np.concatenate([X_win, time_encoding], axis=1) \n",
    "    y_future = ratechg[i + window : i + window + horizon]     # shape: [horizon]\n",
    "    X_list.append(X_win)\n",
    "    y_list.append(y_future)\n",
    "\n",
    "trainX = np.array(X_list[:-n_test])\n",
    "trainy = np.array(y_list[:-n_test])\n",
    "testX = np.array(X_list[-n_test:])\n",
    "testy = np.array(y_list[-n_test:])\n",
    "\n",
    "# trainX.shape: (2146, 15, 61)  2146 = num_samples - window - horizon + 1\n",
    "# trainy.shape: (2146, 10)\n",
    "# testX.shape: (350, 15, 61)\n",
    "# testy.shape: (350, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc2b81",
   "metadata": {},
   "source": [
    "#### train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1ed3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(len(trainX) * val_ratio)\n",
    "\n",
    "valX = trainX[-val_size:]\n",
    "valy = trainy[-val_size:]\n",
    "\n",
    "trainX = trainX[:-val_size]\n",
    "trainy = trainy[:-val_size]\n",
    "\n",
    "# trainX.shape: (1717, 15, 61)\n",
    "# trainy.shape: (1717, 10)\n",
    "# valX.shape: (429, 15, 61)\n",
    "# valy.shape: (429, 10)\n",
    "\n",
    "in_dim = trainX.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98053803",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_tensor = torch.tensor(trainX, dtype=torch.float32)\n",
    "trainy_tensor = torch.tensor(trainy, dtype=torch.float32)\n",
    "\n",
    "valX_tensor = torch.tensor(valX, dtype=torch.float32)\n",
    "valy_tensor = torch.tensor(valy, dtype=torch.float32)\n",
    "\n",
    "testX_tensor = torch.tensor(testX, dtype=torch.float32)\n",
    "testy_tensor = torch.tensor(testy, dtype=torch.float32)\n",
    "\n",
    "# Build datasets and loaders\n",
    "train_dataset = TensorDataset(trainX_tensor, trainy_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(valX_tensor, valy_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(testX_tensor, testy_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76877df",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e507a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    多層 Mamba 時序模型，用於序列輸入特徵的未來預測。\n",
    "\n",
    "    參數:\n",
    "        in_dim (int): features 數量\n",
    "        horizon (int): 模型預測的未來時間步數（輸出維度）。\n",
    "        hidden (int): 中間表示層（latent layer）的維度。\n",
    "        layer (int): Mamba block 的堆疊層數。\n",
    "\n",
    "    屬性:\n",
    "        config (MambaConfig): 用於初始化 Mamba 模型的配置。\n",
    "        proj_in (nn.Linear): 將輸入特徵從 in_dim 投影到 hidden 維度。\n",
    "        mamba (Mamba): 多層 Mamba 模型，用於處理時間序列資料。\n",
    "        pool (nn.AdaptiveAvgPool1d): 算平均\n",
    "        head (nn.Linear): 轉換為 horizon 維度的預測向量。\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    horizon: int\n",
    "    hidden: int\n",
    "    layer: int\n",
    "\n",
    "    config: MambaConfig\n",
    "    proj_in: nn.Linear\n",
    "    mamba: Mamba\n",
    "    pool: nn.AdaptiveAvgPool1d\n",
    "    head: nn.Linear\n",
    "\n",
    "    def __init__(self, in_dim: int, horizon: int, hidden: int, layer: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.horizon = horizon\n",
    "        self.hidden = hidden\n",
    "        self.layer = layer\n",
    "\n",
    "        self.config = MambaConfig(d_model=hidden, n_layers=layer)\n",
    "        self.proj_in = nn.Linear(in_dim, hidden)\n",
    "        self.mamba = Mamba(self.config)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        x = self.proj_in(x)            # x: [B, window, in_dim] → [B, window, hidden]\n",
    "        x = self.mamba(x)              #                        → [B, window, hidden]\n",
    "        x = x.transpose(1, 2)          #                        → [B, hidden, window]\n",
    "        x = self.pool(x).squeeze(-1)   #                        → [B, hidden]\n",
    "        return self.head(x)            #                        → [B, horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1031b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected MLP 模型，用於序列特徵的未來預測。\n",
    "\n",
    "    每個時間步的特徵會被展平成一個長向量，送進 MLP 層中做預測。\n",
    "\n",
    "    輸入:\n",
    "        x: [B, window, in_dim] → 展平 → [B, window * in_dim]\n",
    "\n",
    "    輸出:\n",
    "        預測向量 [B, horizon]\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    horizon: int\n",
    "    hidden: int\n",
    "    window: int\n",
    "    \n",
    "    def __init__(self, in_dim: int, horizon: int, hidden: int, window: int) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten_dim = window * in_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)  # [B, window * in_dim]\n",
    "        return self.fc(x)          # → [B, horizon]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc3dad",
   "metadata": {},
   "source": [
    "#### dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d027d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataBatch:\n",
    "    train_loader: DataLoader\n",
    "    val_loader: DataLoader\n",
    "    test_loader: DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e3ea5",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8901e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson(pred, target):\n",
    "    pred_np = pred.detach().cpu().view(-1).numpy().astype(np.float64)\n",
    "    target_np = target.detach().cpu().view(-1).numpy().astype(np.float64)\n",
    "    return pearsonr(pred_np, target_np)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739802fa",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf8627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    device: str \n",
    "    model: torch.nn.Module\n",
    "    loss_fn: str\n",
    "    opt: torch.optim.Optimizer\n",
    "    train_losses: list\n",
    "    val_losses: list\n",
    "    \n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, loss_fn: str, opt: torch.optim.Optimizer, scheduler, device: str = \"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = get_loss_fn(loss_fn)\n",
    "        self.opt = opt\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        databatch: DataBatch,\n",
    "        epochs: int,\n",
    "        verbose: bool = True,\n",
    "        filename: Optional[str] = None,\n",
    "        patience: int = 10\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        counter = 0\n",
    "        \n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        best_epoch = -1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "\n",
    "            for X_batch, y_batch in databatch.train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "\n",
    "                pred = self.model(X_batch)\n",
    "                loss = self.loss_fn(pred, y_batch)\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss_total = 0.0\n",
    "                for valX, valy in databatch.val_loader:\n",
    "                    valX = valX.to(self.device)\n",
    "                    valy = valy.to(self.device)\n",
    "                    val_pred = self.model(valX)\n",
    "                    val_loss_total += self.loss_fn(val_pred, valy).item()\n",
    "                val_loss_avg = val_loss_total / len(databatch.val_loader)\n",
    "\n",
    "            self.train_losses.append(epoch_train_loss / len(databatch.train_loader))\n",
    "            self.val_losses.append(val_loss_avg)\n",
    "            self.scheduler.step(val_loss_avg)\n",
    "            \n",
    "\n",
    "            current_lr = self.opt.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch}: lr={current_lr:.6f}, ValLoss={val_loss_avg:.8f}\")\n",
    "\n",
    "            if val_loss_avg < best_val_loss:\n",
    "                best_val_loss = val_loss_avg\n",
    "                best_model_state = self.model.state_dict()\n",
    "                best_epoch = epoch\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"[Epoch {epoch+1}] Train: {epoch_train_loss / len(databatch.train_loader):.4f} | Val: {val_loss_avg:.4f}\")\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        if filename:\n",
    "            path = f\"{filename}_{timestamp}.pth\"\n",
    "        else:\n",
    "            path = f\"checkpoint_{timestamp}.pth\"       \n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "        \"model_state\": best_model_state,\n",
    "        \"epoch\": best_epoch,\n",
    "        \"val_loss\": best_val_loss\n",
    "        }, path)\n",
    "        \n",
    "        print(f\"Best checkpoint saved at epoch {best_epoch+1}, Val Loss: {best_val_loss:.4f}\")\n",
    "        self.model.load_state_dict(best_model_state)\n",
    "    \n",
    "        preds, targets, test_loss, r2 = self.evaluate(databatch)\n",
    "        pearson_corr = compute_pearson(preds, targets)\n",
    "\n",
    "        log_entry = (\n",
    "            f\"{path}\\n\"\n",
    "            f\"R²: {r2:.4f} | Pearson: {pearson_corr:.4f}\\n\"\n",
    "            f\"{'-'*35}\\n\"\n",
    "        )\n",
    "\n",
    "        with open(\"training_log.txt\", \"a\") as f:\n",
    "            f.write(log_entry)\n",
    "        \n",
    "        return self.train_losses, self.val_losses\n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        databatch: DataBatch,\n",
    "    ) -> Tuple[torch.Tensor, float]:\n",
    "\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for testX, testy in databatch.test_loader:\n",
    "                testX = testX.to(self.device)\n",
    "                testy = testy.to(self.device)\n",
    "\n",
    "                pred = self.model(testX)\n",
    "                loss = self.loss_fn(pred, testy)\n",
    "\n",
    "                all_preds.append(pred.cpu())\n",
    "                all_targets.append(testy.cpu())\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        preds = torch.cat(all_preds)\n",
    "        targets = torch.cat(all_targets)\n",
    "        r2 = r2_score(targets, preds)\n",
    "        avg_loss = total_loss / len(databatch.test_loader)\n",
    "\n",
    "        print(f\"Test Loss: {avg_loss:.4f} | R² Score: {r2:.4f}\")\n",
    "        return preds, targets, avg_loss, r2\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"val_losses\": self.val_losses\n",
    "        }, path)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1a501",
   "metadata": {},
   "source": [
    "#### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfbd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: lr=0.000500, ValLoss=0.00016115\n",
      "Epoch 1: lr=0.000500, ValLoss=0.00015892\n",
      "Epoch 2: lr=0.000500, ValLoss=0.00016789\n",
      "Epoch 3: lr=0.000500, ValLoss=0.00016255\n",
      "Epoch 4: lr=0.000500, ValLoss=0.00016028\n",
      "Epoch 5: lr=0.000500, ValLoss=0.00016149\n",
      "Epoch 6: lr=0.000500, ValLoss=0.00016686\n",
      "Epoch 7: lr=0.000250, ValLoss=0.00016275\n",
      "Epoch 8: lr=0.000250, ValLoss=0.00016007\n",
      "Epoch 9: lr=0.000250, ValLoss=0.00016205\n",
      "Epoch 10: lr=0.000250, ValLoss=0.00015893\n",
      "Epoch 11: lr=0.000250, ValLoss=0.00016261\n",
      "Early stopping triggered at epoch 12\n",
      "Best checkpoint saved at epoch 2, Val Loss: 0.0002\n",
      "Test Loss: 0.0003 | R² Score: -0.0152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.004053785001849734,\n",
       "  0.00012662085790444112,\n",
       "  0.0001266457106537001,\n",
       "  0.0001264566976801088,\n",
       "  0.00012565660956238817,\n",
       "  0.00012647017666615352,\n",
       "  0.00012703021227180043,\n",
       "  0.000127888334182504,\n",
       "  0.0001235702438862063,\n",
       "  0.00012478622313279148,\n",
       "  0.0001248644220363788,\n",
       "  0.00012476851688401403],\n",
       " [0.00016114591906241712,\n",
       "  0.00015891716564645457,\n",
       "  0.00016788772936032965,\n",
       "  0.00016254696169602944,\n",
       "  0.00016028373486026086,\n",
       "  0.00016148765327536103,\n",
       "  0.00016686107078890523,\n",
       "  0.00016274588812882062,\n",
       "  0.00016007227269357018,\n",
       "  0.00016205029634066805,\n",
       "  0.00015893236101234193,\n",
       "  0.00016261404892214127])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = DataBatch(\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    ")\n",
    "\n",
    "regressor = Net(in_dim=in_dim, horizon=n_steps, hidden=hidden, layer=layer)\n",
    "opt = torch.optim.Adam(regressor.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5)\n",
    "trainer = Trainer(model=regressor, loss_fn=\"smooth\", opt=opt, scheduler=scheduler, device=\"cuda\")\n",
    "trainer.fit(databatch=databatch, epochs=epochs, verbose=False, filename=\"0730_2330/2330\", patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec205adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambastock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

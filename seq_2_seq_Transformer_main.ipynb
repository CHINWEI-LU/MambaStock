{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3892de",
   "metadata": {},
   "source": [
    "#### package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8089c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchinwei/anaconda3/envs/mambastock/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Callable, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mamba import Mamba, MambaConfig\n",
    "from loss_functions import get_loss_fn\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90293414",
   "metadata": {},
   "source": [
    "#### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fa9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '0730_2330/'\n",
    "use_cuda = True\n",
    "n_steps=10\n",
    "window=30\n",
    "patience = 100\n",
    "val_ratio = 0.2\n",
    "epochs = 1000\n",
    "loss_fcn = \"mse\"\n",
    "\n",
    "lr = 0.0005\n",
    "wd = 1e-5\n",
    "hidden = 32\n",
    "layer = 3\n",
    "n_test = 350\n",
    "ts_code = 2330\n",
    "risk_free = 0.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4616fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_return_features(data, price_col='close', windows=[5, 10, 20]):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = data.copy()\n",
    "    returns = df[price_col].pct_change()\n",
    "\n",
    "    # 基本報酬率\n",
    "    df['returns'] = returns\n",
    "\n",
    "    for window in windows:\n",
    "        # 移動平均報酬率\n",
    "        df[f'returns_ma_{window}'] = returns.rolling(window=window).mean()\n",
    "        \n",
    "        # 指數移動平均\n",
    "        df[f'returns_ema_{window}'] = returns.ewm(span=window, adjust=False).mean()\n",
    "\n",
    "        # 報酬率波動度\n",
    "        df[f'returns_volatility_{window}'] = returns.rolling(window=window).std()\n",
    "\n",
    "        # 報酬率的 Z-score\n",
    "        mean = returns.rolling(window=window).mean()\n",
    "        std = returns.rolling(window=window).std()\n",
    "        df[f'returns_zscore_{window}'] = (returns - mean) / std\n",
    "\n",
    "    return df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ac912",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5320500",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(str(ts_code)+\"_value\"+'.csv')\n",
    "data['trade_date'] = pd.to_datetime(data['trade_date'], format='%Y/%m/%d')\n",
    "\n",
    "# 加入星期幾（0=星期一, 6=星期日）\n",
    "data['day_of_week'] = data['trade_date'].dt.dayofweek\n",
    "\n",
    "# 加入月份（1～12）\n",
    "data['month'] = data['trade_date'].dt.month\n",
    "\n",
    "returns = data['close_TW_roc'] \n",
    "\n",
    "tp=[5, 10, 20]\n",
    "\n",
    "for tp in tp:\n",
    "    # 移動平均報酬率\n",
    "    data[f'returns_ma_{tp}'] = returns.rolling(window=tp).mean()\n",
    "    \n",
    "    # 指數移動平均\n",
    "    data[f'returns_ema_{tp}'] = returns.ewm(span=tp, adjust=False).mean()\n",
    "\n",
    "    # 報酬率波動度\n",
    "    data[f'returns_volatility_{tp}'] = returns.rolling(window=tp).std()\n",
    "\n",
    "    # 報酬率的 Z-score\n",
    "    mean = returns.rolling(window=tp).mean()\n",
    "    std = returns.rolling(window=tp).std()\n",
    "    data[f'returns_zscore_{tp}'] = (returns - mean) / std\n",
    "\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "['trade_date', 'stock_no', 'stock_name', 'stock_type', \n",
    "'open_TW', 'close_TW', close_TW_roc, 'h_TW', 'l_TW', 'vol_TW', 'CBOE_SKEW_INDEX', \n",
    "'CBOE_Volatility_INDEX', 'COPPER_F', 'COPPER', 'GOLD_F', \n",
    "'GOLD', 'OIL_F', 'OIL', 'SP_F', 'SILVER_F', 'SILVER', \n",
    "'TWII', 'IXIC', 'GSPC', 'DJI', 'NYSE', 'RUSSELL', 'SSE', \n",
    "'FCHI', 'FTSE', 'GDAXI', 'Nikkei_F', 'IXIC_F', 'DJI_F', \n",
    "'S_P_F', 'USDX_F', 'JPY', 'GBP', 'HKD', 'CNY', 'AUD', \n",
    "'TWD', 'EUR', 'DTB4WK', 'DTB3', 'DTB6', 'DGS5', 'DGS1', \n",
    "'DAAA', 'DBAA', 'DGS3MO', 'DGS6MO', 'DGS1.1', 'DCOILWTICO', \n",
    "'TE1', 'TE2', 'TE3', 'TE5', 'TE6', 'DE1', 'DE2', 'DE4', 'DE5', \n",
    "'DE6', 'DCOILWTICO_rel_change', 'day_of_week', 'month']\n",
    "'''\n",
    "\n",
    "index = ['open_TW', 'close_TW', 'h_TW', 'l_TW', 'vol_TW', 'CBOE_SKEW_INDEX', \n",
    "'CBOE_Volatility_INDEX', 'TWII', 'DJI', 'NYSE', 'TWD', 'CNY', 'day_of_week', 'month',\n",
    "\"returns_ma_5\", \"returns_ema_5\", \"returns_volatility_5\",\"returns_zscore_5\",\n",
    "\"returns_ma_10\", \"returns_ema_10\", \"returns_volatility_10\", \"returns_zscore_10\",\n",
    "\"returns_ma_20\", \"returns_ema_20\", \"returns_volatility_20\", \"returns_zscore_20\"]\n",
    "\n",
    "# 拆離 label 欄位\n",
    "\n",
    "ratechg = data['close_TW_roc'].values\n",
    "\n",
    "features = data[index].values\n",
    "num_samples = features.shape[0]\n",
    "# num_samples: 2520\n",
    "\n",
    "# 定義 horizon 數\n",
    "horizon = n_steps\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for i in range(num_samples - window - horizon + 1):\n",
    "    X_win = features[i : i + window]                        # shape: [window, features]\n",
    "    t = np.arange(window).reshape(-1, 1)  # shape: [window, 1]\n",
    "    sin_t = np.sin(t / 10000)\n",
    "    cos_t = np.cos(t / 10000)\n",
    "    time_encoding = np.concatenate([sin_t, cos_t], axis=1)    # shape: [window, 2]\n",
    "\n",
    "    X_win = np.concatenate([X_win, time_encoding], axis=1) \n",
    "    y_future = ratechg[i + window : i + window + horizon]     # shape: [horizon]\n",
    "    X_list.append(X_win)\n",
    "    y_list.append(y_future)\n",
    "\n",
    "trainX = np.array(X_list[:-n_test])\n",
    "trainy = np.array(y_list[:-n_test])\n",
    "testX = np.array(X_list[-n_test:])\n",
    "testy = np.array(y_list[-n_test:])\n",
    "\n",
    "# trainX.shape: (2146, 15, 61)  2146 = num_samples - window - horizon + 1\n",
    "# trainy.shape: (2146, 10)\n",
    "# testX.shape: (350, 15, 61)\n",
    "# testy.shape: (350, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc2b81",
   "metadata": {},
   "source": [
    "#### train / val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1ed3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(len(trainX) * val_ratio)\n",
    "\n",
    "valX = trainX[-val_size:]\n",
    "valy = trainy[-val_size:]\n",
    "\n",
    "trainX = trainX[:-val_size]\n",
    "trainy = trainy[:-val_size]\n",
    "\n",
    "# trainX.shape: (1717, 15, 61)\n",
    "# trainy.shape: (1717, 10)\n",
    "# valX.shape: (429, 15, 61)\n",
    "# valy.shape: (429, 10)\n",
    "\n",
    "in_dim = trainX.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98053803",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4a4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_tensor = torch.tensor(trainX, dtype=torch.float32)\n",
    "trainy_tensor = torch.tensor(trainy, dtype=torch.float32)\n",
    "\n",
    "valX_tensor = torch.tensor(valX, dtype=torch.float32)\n",
    "valy_tensor = torch.tensor(valy, dtype=torch.float32)\n",
    "\n",
    "testX_tensor = torch.tensor(testX, dtype=torch.float32)\n",
    "testy_tensor = torch.tensor(testy, dtype=torch.float32)\n",
    "\n",
    "# Build datasets and loaders\n",
    "train_dataset = TensorDataset(trainX_tensor, trainy_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(valX_tensor, valy_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(testX_tensor, testy_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76877df",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e507a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    多層 Mamba 時序模型，用於序列輸入特徵的未來預測。\n",
    "\n",
    "    參數:\n",
    "        in_dim (int): features 數量\n",
    "        horizon (int): 模型預測的未來時間步數（輸出維度）。\n",
    "        hidden (int): 中間表示層（latent layer）的維度。\n",
    "        layer (int): Mamba block 的堆疊層數。\n",
    "\n",
    "    屬性:\n",
    "        config (MambaConfig): 用於初始化 Mamba 模型的配置。\n",
    "        proj_in (nn.Linear): 將輸入特徵從 in_dim 投影到 hidden 維度。\n",
    "        mamba (Mamba): 多層 Mamba 模型，用於處理時間序列資料。\n",
    "        pool (nn.AdaptiveAvgPool1d): 算平均\n",
    "        head (nn.Linear): 轉換為 horizon 維度的預測向量。\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    horizon: int\n",
    "    hidden: int\n",
    "    layer: int\n",
    "\n",
    "    config: MambaConfig\n",
    "    proj_in: nn.Linear\n",
    "    mamba: Mamba\n",
    "    pool: nn.AdaptiveAvgPool1d\n",
    "    head: nn.Linear\n",
    "\n",
    "    def __init__(self, in_dim: int, horizon: int, hidden: int, layer: int) -> None:\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.horizon = horizon\n",
    "        self.hidden = hidden\n",
    "        self.layer = layer\n",
    "\n",
    "        self.config = MambaConfig(d_model=hidden, n_layers=layer)\n",
    "        self.proj_in = nn.Linear(in_dim, hidden)\n",
    "        self.mamba = Mamba(self.config)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(hidden, horizon)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        x = self.proj_in(x)            # x: [B, window, in_dim] → [B, window, hidden]\n",
    "        x = self.mamba(x)              #                        → [B, window, hidden]\n",
    "        x = x.transpose(1, 2)          #                        → [B, hidden, window]\n",
    "        x = self.pool(x).squeeze(-1)   #                        → [B, hidden]\n",
    "        return self.head(x)            #                        → [B, horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1031b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected MLP 模型，用於序列特徵的未來預測。\n",
    "\n",
    "    每個時間步的特徵會被展平成一個長向量，送進 MLP 層中做預測。\n",
    "\n",
    "    輸入:\n",
    "        x: [B, window, in_dim] → 展平 → [B, window * in_dim]\n",
    "\n",
    "    輸出:\n",
    "        預測向量 [B, horizon]\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    horizon: int\n",
    "    hidden: int\n",
    "    window: int\n",
    "    flatten_dim: int\n",
    "    fc: nn.Sequential\n",
    "    \n",
    "    def __init__(self, in_dim: int, horizon: int, hidden: int, window: int) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten_dim: int = window * in_dim\n",
    "        self.fc: nn.Sequential = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)  # [B, window * in_dim]\n",
    "        return self.fc(x)          # → [B, horizon]\n",
    "    \n",
    "    \n",
    "    \n",
    "class TransformerNet(nn.Module):\n",
    "    \"\"\"\n",
    "    輸入:\n",
    "        x: [B, window, in_dim]\n",
    "    輸出:\n",
    "        [B, horizon]\n",
    "    \"\"\"\n",
    "    in_dim: int\n",
    "    hidden: int\n",
    "    window: int\n",
    "    horizon: int\n",
    "    nhead: int\n",
    "    num_layers: int\n",
    "    dropout: float\n",
    "    \n",
    "    def __init__(self, in_dim: int, hidden: int, window: int, horizon: int,\n",
    "                nhead: int = 4, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=in_dim, nhead=nhead,\n",
    "            dim_feedforward=hidden,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),  # [B, window * in_dim]\n",
    "            nn.Linear(window * in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, horizon)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.transpose(0, 1)  # → [seq_len, batch, in_dim]\n",
    "        x = self.encoder(x)\n",
    "        x = x.transpose(0, 1)  # → [batch, seq_len, in_dim] 回來再做 flatten/head  \n",
    "        out = self.head(x)   \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc3dad",
   "metadata": {},
   "source": [
    "#### dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d027d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataBatch:\n",
    "    train_loader: DataLoader\n",
    "    val_loader: DataLoader\n",
    "    test_loader: DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e3ea5",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8901e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson(pred, target):\n",
    "    pred_np = pred.detach().cpu().view(-1).numpy().astype(np.float64)\n",
    "    target_np = target.detach().cpu().view(-1).numpy().astype(np.float64)\n",
    "    return pearsonr(pred_np, target_np)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739802fa",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf8627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    device: str \n",
    "    model: torch.nn.Module\n",
    "    loss_fn: str\n",
    "    opt: torch.optim.Optimizer\n",
    "    train_losses: list\n",
    "    val_losses: list\n",
    "    \n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, loss_fn: str, opt: torch.optim.Optimizer, scheduler, device: str = \"cuda\"):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = get_loss_fn(loss_fn)\n",
    "        self.opt = opt\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        databatch: DataBatch,\n",
    "        epochs: int,\n",
    "        verbose: bool = True,\n",
    "        filename: Optional[str] = None,\n",
    "        patience: int = 10\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        counter = 0\n",
    "        \n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        best_epoch = -1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "\n",
    "            for X_batch, y_batch in databatch.train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "\n",
    "                pred = self.model(X_batch)\n",
    "                loss = self.loss_fn(pred, y_batch)\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss_total = 0.0\n",
    "                for valX, valy in databatch.val_loader:\n",
    "                    valX = valX.to(self.device)\n",
    "                    valy = valy.to(self.device)\n",
    "                    val_pred = self.model(valX)\n",
    "                    val_loss_total += self.loss_fn(val_pred, valy).item()\n",
    "                val_loss_avg = val_loss_total / len(databatch.val_loader)\n",
    "\n",
    "            self.train_losses.append(epoch_train_loss / len(databatch.train_loader))\n",
    "            self.val_losses.append(val_loss_avg)\n",
    "            self.scheduler.step(val_loss_avg)\n",
    "            \n",
    "\n",
    "            current_lr = self.opt.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch}: lr={current_lr:.6f}, ValLoss={val_loss_avg:.8f}\")\n",
    "\n",
    "            if val_loss_avg < best_val_loss:\n",
    "                best_val_loss = val_loss_avg\n",
    "                best_model_state = self.model.state_dict()\n",
    "                best_epoch = epoch\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"[Epoch {epoch+1}] Train: {epoch_train_loss / len(databatch.train_loader):.4f} | Val: {val_loss_avg:.4f}\")\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        if filename:\n",
    "            path = f\"{filename}_{timestamp}.pth\"\n",
    "        else:\n",
    "            path = f\"checkpoint_{timestamp}.pth\"       \n",
    "        \n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "        \"model_state\": best_model_state,\n",
    "        \"epoch\": best_epoch,\n",
    "        \"val_loss\": best_val_loss\n",
    "        }, path)\n",
    "        \n",
    "        print(f\"Best checkpoint saved at epoch {best_epoch+1}, Val Loss: {best_val_loss:.4f}\")\n",
    "        self.model.load_state_dict(best_model_state)\n",
    "    \n",
    "        preds, targets, test_loss, r2 = self.evaluate(databatch)\n",
    "        pearson_corr = compute_pearson(preds, targets)\n",
    "\n",
    "        log_entry = (\n",
    "            f\"{path}\\n\"\n",
    "            f\"R²: {r2:.4f} | Pearson: {pearson_corr:.4f}\\n\"\n",
    "            f\"{'-'*35}\\n\"\n",
    "        )\n",
    "\n",
    "        with open(\"training_log.txt\", \"a\") as f:\n",
    "            f.write(log_entry)\n",
    "        \n",
    "        return self.train_losses, self.val_losses\n",
    "\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        databatch: DataBatch,\n",
    "    ) -> Tuple[torch.Tensor, float]:\n",
    "\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for testX, testy in databatch.test_loader:\n",
    "                testX = testX.to(self.device)\n",
    "                testy = testy.to(self.device)\n",
    "\n",
    "                pred = self.model(testX)\n",
    "                loss = self.loss_fn(pred, testy)\n",
    "\n",
    "                all_preds.append(pred.cpu())\n",
    "                all_targets.append(testy.cpu())\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        preds = torch.cat(all_preds)\n",
    "        targets = torch.cat(all_targets)\n",
    "        r2 = r2_score(targets, preds)\n",
    "        avg_loss = total_loss / len(databatch.test_loader)\n",
    "\n",
    "        print(f\"Test Loss: {avg_loss:.4f} | R² Score: {r2:.4f}\")\n",
    "        return preds, targets, avg_loss, r2\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"train_losses\": self.train_losses,\n",
    "            \"val_losses\": self.val_losses\n",
    "        }, path)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1a501",
   "metadata": {},
   "source": [
    "#### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfbd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: lr=0.000500, ValLoss=0.00047961\n",
      "Epoch 1: lr=0.000500, ValLoss=0.00045013\n",
      "Epoch 2: lr=0.000500, ValLoss=0.00035380\n",
      "Epoch 3: lr=0.000500, ValLoss=0.00036158\n",
      "Epoch 4: lr=0.000500, ValLoss=0.00032314\n",
      "Epoch 5: lr=0.000500, ValLoss=0.00032823\n",
      "Epoch 6: lr=0.000500, ValLoss=0.00032369\n",
      "Epoch 7: lr=0.000500, ValLoss=0.00032194\n",
      "Epoch 8: lr=0.000500, ValLoss=0.00033268\n",
      "Epoch 9: lr=0.000500, ValLoss=0.00032350\n",
      "Epoch 10: lr=0.000500, ValLoss=0.00032339\n",
      "Epoch 11: lr=0.000500, ValLoss=0.00032365\n",
      "Epoch 12: lr=0.000500, ValLoss=0.00031862\n",
      "Epoch 13: lr=0.000500, ValLoss=0.00032222\n",
      "Epoch 14: lr=0.000500, ValLoss=0.00032243\n",
      "Epoch 15: lr=0.000500, ValLoss=0.00033093\n",
      "Epoch 16: lr=0.000500, ValLoss=0.00032392\n",
      "Epoch 17: lr=0.000500, ValLoss=0.00031985\n",
      "Epoch 18: lr=0.000500, ValLoss=0.00036183\n",
      "Epoch 19: lr=0.000500, ValLoss=0.00032418\n",
      "Epoch 20: lr=0.000500, ValLoss=0.00033934\n",
      "Epoch 21: lr=0.000500, ValLoss=0.00032401\n",
      "Epoch 22: lr=0.000500, ValLoss=0.00031960\n",
      "Epoch 23: lr=0.000250, ValLoss=0.00033417\n",
      "Epoch 24: lr=0.000250, ValLoss=0.00031732\n",
      "Epoch 25: lr=0.000250, ValLoss=0.00032552\n",
      "Epoch 26: lr=0.000250, ValLoss=0.00032262\n",
      "Epoch 27: lr=0.000250, ValLoss=0.00032349\n",
      "Epoch 28: lr=0.000250, ValLoss=0.00033185\n",
      "Epoch 29: lr=0.000250, ValLoss=0.00032819\n",
      "Epoch 30: lr=0.000250, ValLoss=0.00031789\n",
      "Epoch 31: lr=0.000250, ValLoss=0.00031797\n",
      "Epoch 32: lr=0.000250, ValLoss=0.00033490\n",
      "Epoch 33: lr=0.000250, ValLoss=0.00032118\n",
      "Epoch 34: lr=0.000250, ValLoss=0.00033264\n",
      "Epoch 35: lr=0.000125, ValLoss=0.00032206\n",
      "Epoch 36: lr=0.000125, ValLoss=0.00031866\n",
      "Epoch 37: lr=0.000125, ValLoss=0.00032193\n",
      "Epoch 38: lr=0.000125, ValLoss=0.00032111\n",
      "Epoch 39: lr=0.000125, ValLoss=0.00032417\n",
      "Epoch 40: lr=0.000125, ValLoss=0.00032221\n",
      "Epoch 41: lr=0.000125, ValLoss=0.00032593\n",
      "Epoch 42: lr=0.000125, ValLoss=0.00032360\n",
      "Epoch 43: lr=0.000125, ValLoss=0.00031925\n",
      "Epoch 44: lr=0.000125, ValLoss=0.00032311\n",
      "Epoch 45: lr=0.000125, ValLoss=0.00032480\n",
      "Epoch 46: lr=0.000063, ValLoss=0.00031861\n",
      "Epoch 47: lr=0.000063, ValLoss=0.00031742\n",
      "Epoch 48: lr=0.000063, ValLoss=0.00031816\n",
      "Epoch 49: lr=0.000063, ValLoss=0.00031786\n",
      "Epoch 50: lr=0.000063, ValLoss=0.00031942\n",
      "Epoch 51: lr=0.000063, ValLoss=0.00031824\n",
      "Epoch 52: lr=0.000063, ValLoss=0.00031806\n",
      "Epoch 53: lr=0.000063, ValLoss=0.00031823\n",
      "Epoch 54: lr=0.000063, ValLoss=0.00031756\n",
      "Epoch 55: lr=0.000063, ValLoss=0.00031737\n",
      "Epoch 56: lr=0.000063, ValLoss=0.00031656\n",
      "Epoch 57: lr=0.000063, ValLoss=0.00032574\n",
      "Epoch 58: lr=0.000063, ValLoss=0.00031861\n",
      "Epoch 59: lr=0.000063, ValLoss=0.00031783\n",
      "Epoch 60: lr=0.000063, ValLoss=0.00032102\n",
      "Epoch 61: lr=0.000063, ValLoss=0.00031785\n",
      "Epoch 62: lr=0.000063, ValLoss=0.00031834\n",
      "Epoch 63: lr=0.000063, ValLoss=0.00031702\n",
      "Epoch 64: lr=0.000063, ValLoss=0.00031707\n",
      "Epoch 65: lr=0.000063, ValLoss=0.00031780\n",
      "Epoch 66: lr=0.000063, ValLoss=0.00032293\n",
      "Epoch 67: lr=0.000031, ValLoss=0.00031909\n",
      "Epoch 68: lr=0.000031, ValLoss=0.00031722\n",
      "Epoch 69: lr=0.000031, ValLoss=0.00031740\n",
      "Epoch 70: lr=0.000031, ValLoss=0.00031811\n",
      "Epoch 71: lr=0.000031, ValLoss=0.00031906\n",
      "Epoch 72: lr=0.000031, ValLoss=0.00031760\n",
      "Epoch 73: lr=0.000031, ValLoss=0.00031668\n",
      "Epoch 74: lr=0.000031, ValLoss=0.00031690\n",
      "Epoch 75: lr=0.000031, ValLoss=0.00031630\n",
      "Epoch 76: lr=0.000031, ValLoss=0.00031946\n",
      "Epoch 77: lr=0.000031, ValLoss=0.00031896\n",
      "Epoch 78: lr=0.000031, ValLoss=0.00031669\n",
      "Epoch 79: lr=0.000031, ValLoss=0.00031808\n",
      "Epoch 80: lr=0.000031, ValLoss=0.00031586\n",
      "Epoch 81: lr=0.000031, ValLoss=0.00031860\n",
      "Epoch 82: lr=0.000031, ValLoss=0.00031819\n",
      "Epoch 83: lr=0.000031, ValLoss=0.00031829\n",
      "Epoch 84: lr=0.000031, ValLoss=0.00031794\n",
      "Epoch 85: lr=0.000031, ValLoss=0.00031795\n",
      "Epoch 86: lr=0.000031, ValLoss=0.00032075\n",
      "Epoch 87: lr=0.000031, ValLoss=0.00031787\n",
      "Epoch 88: lr=0.000031, ValLoss=0.00031788\n",
      "Epoch 89: lr=0.000031, ValLoss=0.00031605\n",
      "Epoch 90: lr=0.000031, ValLoss=0.00031751\n",
      "Epoch 91: lr=0.000016, ValLoss=0.00031687\n",
      "Epoch 92: lr=0.000016, ValLoss=0.00031619\n",
      "Epoch 93: lr=0.000016, ValLoss=0.00031617\n",
      "Epoch 94: lr=0.000016, ValLoss=0.00031857\n",
      "Epoch 95: lr=0.000016, ValLoss=0.00031680\n",
      "Epoch 96: lr=0.000016, ValLoss=0.00031602\n",
      "Epoch 97: lr=0.000016, ValLoss=0.00031656\n",
      "Epoch 98: lr=0.000016, ValLoss=0.00031764\n",
      "Epoch 99: lr=0.000016, ValLoss=0.00031670\n",
      "Epoch 100: lr=0.000016, ValLoss=0.00031635\n",
      "Epoch 101: lr=0.000016, ValLoss=0.00031628\n",
      "Epoch 102: lr=0.000008, ValLoss=0.00031653\n",
      "Epoch 103: lr=0.000008, ValLoss=0.00031615\n",
      "Epoch 104: lr=0.000008, ValLoss=0.00031599\n",
      "Epoch 105: lr=0.000008, ValLoss=0.00031616\n",
      "Epoch 106: lr=0.000008, ValLoss=0.00031615\n",
      "Epoch 107: lr=0.000008, ValLoss=0.00031654\n",
      "Epoch 108: lr=0.000008, ValLoss=0.00031661\n",
      "Epoch 109: lr=0.000008, ValLoss=0.00031694\n",
      "Epoch 110: lr=0.000008, ValLoss=0.00031607\n",
      "Epoch 111: lr=0.000008, ValLoss=0.00031646\n",
      "Epoch 112: lr=0.000008, ValLoss=0.00031679\n",
      "Epoch 113: lr=0.000004, ValLoss=0.00031666\n",
      "Epoch 114: lr=0.000004, ValLoss=0.00031618\n",
      "Epoch 115: lr=0.000004, ValLoss=0.00031654\n",
      "Epoch 116: lr=0.000004, ValLoss=0.00031648\n",
      "Epoch 117: lr=0.000004, ValLoss=0.00031609\n",
      "Epoch 118: lr=0.000004, ValLoss=0.00031621\n",
      "Epoch 119: lr=0.000004, ValLoss=0.00031697\n",
      "Epoch 120: lr=0.000004, ValLoss=0.00031624\n",
      "Epoch 121: lr=0.000004, ValLoss=0.00031622\n",
      "Epoch 122: lr=0.000004, ValLoss=0.00031620\n",
      "Epoch 123: lr=0.000004, ValLoss=0.00031604\n",
      "Epoch 124: lr=0.000002, ValLoss=0.00031613\n",
      "Epoch 125: lr=0.000002, ValLoss=0.00031613\n",
      "Epoch 126: lr=0.000002, ValLoss=0.00031608\n",
      "Epoch 127: lr=0.000002, ValLoss=0.00031608\n",
      "Epoch 128: lr=0.000002, ValLoss=0.00031601\n",
      "Epoch 129: lr=0.000002, ValLoss=0.00031602\n",
      "Epoch 130: lr=0.000002, ValLoss=0.00031615\n",
      "Epoch 131: lr=0.000002, ValLoss=0.00031631\n",
      "Epoch 132: lr=0.000002, ValLoss=0.00031625\n",
      "Epoch 133: lr=0.000002, ValLoss=0.00031639\n",
      "Epoch 134: lr=0.000002, ValLoss=0.00031618\n",
      "Epoch 135: lr=0.000001, ValLoss=0.00031645\n",
      "Epoch 136: lr=0.000001, ValLoss=0.00031615\n",
      "Epoch 137: lr=0.000001, ValLoss=0.00031607\n",
      "Epoch 138: lr=0.000001, ValLoss=0.00031610\n",
      "Epoch 139: lr=0.000001, ValLoss=0.00031610\n",
      "Epoch 140: lr=0.000001, ValLoss=0.00031609\n",
      "Epoch 141: lr=0.000001, ValLoss=0.00031609\n",
      "Epoch 142: lr=0.000001, ValLoss=0.00031608\n",
      "Epoch 143: lr=0.000001, ValLoss=0.00031611\n",
      "Epoch 144: lr=0.000001, ValLoss=0.00031609\n",
      "Epoch 145: lr=0.000001, ValLoss=0.00031617\n",
      "Epoch 146: lr=0.000000, ValLoss=0.00031607\n",
      "Epoch 147: lr=0.000000, ValLoss=0.00031610\n",
      "Epoch 148: lr=0.000000, ValLoss=0.00031604\n",
      "Epoch 149: lr=0.000000, ValLoss=0.00031607\n",
      "Epoch 150: lr=0.000000, ValLoss=0.00031606\n",
      "Early stopping triggered at epoch 151\n",
      "Best checkpoint saved at epoch 81, Val Loss: 0.0003\n",
      "Test Loss: 0.0005 | R² Score: -0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.002720820286996522,\n",
       "  0.0007193696005313815,\n",
       "  0.0005058116432749403,\n",
       "  0.0004283021838966546,\n",
       "  0.00040017280624388667,\n",
       "  0.00035210196609731836,\n",
       "  0.0003344077602652657,\n",
       "  0.00031410171449918053,\n",
       "  0.0002964575371907075,\n",
       "  0.0003030503204120938,\n",
       "  0.0002804370642308023,\n",
       "  0.00027751084513202276,\n",
       "  0.000277790288031233,\n",
       "  0.0002695403678478884,\n",
       "  0.0002730768824650748,\n",
       "  0.0002684100597760781,\n",
       "  0.00026292155083831187,\n",
       "  0.0002639233310408187,\n",
       "  0.0002675242116619868,\n",
       "  0.00026224620359123875,\n",
       "  0.0002589449204091545,\n",
       "  0.0002657134134766101,\n",
       "  0.0002596334240976985,\n",
       "  0.00025757799242987774,\n",
       "  0.000253732050692153,\n",
       "  0.0002492358638984063,\n",
       "  0.0002518669473675062,\n",
       "  0.00024977507633113145,\n",
       "  0.0002538521546499259,\n",
       "  0.00025301010821251866,\n",
       "  0.0002515543701691676,\n",
       "  0.0002523940439579815,\n",
       "  0.0002566891703710733,\n",
       "  0.0002504964559536673,\n",
       "  0.00025202273962281224,\n",
       "  0.00025438779664669573,\n",
       "  0.00024904096405733517,\n",
       "  0.00024683998385883587,\n",
       "  0.0002473791979389476,\n",
       "  0.0002463649685729629,\n",
       "  0.0002466969095655928,\n",
       "  0.0002475635584867735,\n",
       "  0.00024739895889069885,\n",
       "  0.0002478664689576517,\n",
       "  0.00024706268746774857,\n",
       "  0.0002464211553023285,\n",
       "  0.0002458547044299402,\n",
       "  0.0002451970321312986,\n",
       "  0.0002446708921233262,\n",
       "  0.00024514182065481775,\n",
       "  0.00024421520480108537,\n",
       "  0.00024478289807755674,\n",
       "  0.0002449582435766165,\n",
       "  0.0002449712485957996,\n",
       "  0.0002444009175787697,\n",
       "  0.0002446666050741871,\n",
       "  0.00024603527351743805,\n",
       "  0.00024484701958190616,\n",
       "  0.00024502027962677495,\n",
       "  0.00024510181785170163,\n",
       "  0.00024473556495306767,\n",
       "  0.00024514391818984395,\n",
       "  0.0002447341340604976,\n",
       "  0.00024518058064200405,\n",
       "  0.0002449860208489897,\n",
       "  0.0002449847441243517,\n",
       "  0.000244825287806617,\n",
       "  0.00024356631842718617,\n",
       "  0.00024378080583256582,\n",
       "  0.00024251647544519555,\n",
       "  0.00024286941622432215,\n",
       "  0.00024294713017847038,\n",
       "  0.00024380250040437359,\n",
       "  0.00024326774319150609,\n",
       "  0.0002429729288022192,\n",
       "  0.0002427440258028548,\n",
       "  0.00024335986395357704,\n",
       "  0.0002426307127846159,\n",
       "  0.00024283803392087443,\n",
       "  0.0002431343777713647,\n",
       "  0.0002425351007984301,\n",
       "  0.00024341424857925203,\n",
       "  0.00024252713142832705,\n",
       "  0.00024321346166477766,\n",
       "  0.00024261252426340262,\n",
       "  0.00024256918110933048,\n",
       "  0.00024352239182368272,\n",
       "  0.00024319917429238558,\n",
       "  0.00024342097087767643,\n",
       "  0.00024207039568177865,\n",
       "  0.00024212526106978503,\n",
       "  0.0002420403002618531,\n",
       "  0.0002418947493973768,\n",
       "  0.0002421111285517361,\n",
       "  0.0002417412158464222,\n",
       "  0.00024192549896707652,\n",
       "  0.0002420134013211179,\n",
       "  0.00024187253438867629,\n",
       "  0.00024154684879764353,\n",
       "  0.00024207179184194724,\n",
       "  0.00024168511670311245,\n",
       "  0.00024247152499090176,\n",
       "  0.0002415737985327998,\n",
       "  0.0002411859582797712,\n",
       "  0.0002408220891495903,\n",
       "  0.00024073993329195973,\n",
       "  0.00024094413688630393,\n",
       "  0.00024142003572053927,\n",
       "  0.00024107981201114116,\n",
       "  0.0002407903311047088,\n",
       "  0.0002409670878649812,\n",
       "  0.00024176534223545977,\n",
       "  0.0002411372727881414,\n",
       "  0.00024086057718070047,\n",
       "  0.00024087974905444943,\n",
       "  0.00024060700964599954,\n",
       "  0.000241451303393334,\n",
       "  0.00024078081113194062,\n",
       "  0.00024107567823759965,\n",
       "  0.00024063199569629329,\n",
       "  0.00024067120500818282,\n",
       "  0.00024052716424811702,\n",
       "  0.0002409488228775719,\n",
       "  0.00024043364801228854,\n",
       "  0.00024110590366378673,\n",
       "  0.00024051367558367587,\n",
       "  0.00024064247074146,\n",
       "  0.0002414667280116294,\n",
       "  0.00024058428463385494,\n",
       "  0.00024060624937706997,\n",
       "  0.00024033060082111438,\n",
       "  0.0002405629439756081,\n",
       "  0.0002408228919760078,\n",
       "  0.00024095336348696982,\n",
       "  0.00024114499601122553,\n",
       "  0.0002408506740532587,\n",
       "  0.00024073324545137234,\n",
       "  0.00024117560335654625,\n",
       "  0.00024083752392676993,\n",
       "  0.00024050248612184078,\n",
       "  0.0002404590271015757,\n",
       "  0.00024060243210768467,\n",
       "  0.00024024541075187648,\n",
       "  0.00024045721827105636,\n",
       "  0.00024031259790546137,\n",
       "  0.0002405209855872804,\n",
       "  0.00024086893286296817,\n",
       "  0.00024011877504339933,\n",
       "  0.00024069294007824523,\n",
       "  0.0002408067679048059,\n",
       "  0.00024087051826667585],\n",
       " [0.0004796106594666425,\n",
       "  0.00045012998021707907,\n",
       "  0.0003537973614045229,\n",
       "  0.00036157804963834306,\n",
       "  0.00032313542477721866,\n",
       "  0.0003282299404856059,\n",
       "  0.0003236876612020381,\n",
       "  0.00032193527187229375,\n",
       "  0.0003326764971360616,\n",
       "  0.0003235022037794503,\n",
       "  0.0003233942520132081,\n",
       "  0.00032364772764763246,\n",
       "  0.0003186185229558894,\n",
       "  0.0003222176392564843,\n",
       "  0.0003224347305216792,\n",
       "  0.0003309323598201714,\n",
       "  0.00032392214618286945,\n",
       "  0.00031984897860739793,\n",
       "  0.00036183441207632077,\n",
       "  0.0003241832202076222,\n",
       "  0.0003393352213876184,\n",
       "  0.00032401039950653084,\n",
       "  0.0003195956144029171,\n",
       "  0.000334167193849916,\n",
       "  0.00031731756462249905,\n",
       "  0.00032552200610990877,\n",
       "  0.0003226196989729673,\n",
       "  0.0003234948211077911,\n",
       "  0.00033184889104665707,\n",
       "  0.00032818536999036077,\n",
       "  0.00031788915981047065,\n",
       "  0.000317972683087983,\n",
       "  0.000334900295220669,\n",
       "  0.0003211819592386881,\n",
       "  0.0003326429107767547,\n",
       "  0.00032206036296513586,\n",
       "  0.0003186635909110515,\n",
       "  0.0003219316660963361,\n",
       "  0.00032111268754104257,\n",
       "  0.00032416804525483813,\n",
       "  0.00032221417751753943,\n",
       "  0.0003259264598859267,\n",
       "  0.0003235952763471114,\n",
       "  0.0003192466338751493,\n",
       "  0.00032310947453542785,\n",
       "  0.00032480113050378686,\n",
       "  0.0003186123529438326,\n",
       "  0.0003174222618798484,\n",
       "  0.00031816195688507933,\n",
       "  0.00031785843465412554,\n",
       "  0.00031941563941439164,\n",
       "  0.00031823763412022445,\n",
       "  0.0003180599746378397,\n",
       "  0.00031823361684844175,\n",
       "  0.000317563902003544,\n",
       "  0.0003173723444989365,\n",
       "  0.0003165609780774469,\n",
       "  0.000325743121922844,\n",
       "  0.0003186115415398187,\n",
       "  0.00031782645911099044,\n",
       "  0.00032101884938095246,\n",
       "  0.00031784919998177364,\n",
       "  0.00031834027982700653,\n",
       "  0.00031702085646059087,\n",
       "  0.00031706621222029853,\n",
       "  0.00031779647259908315,\n",
       "  0.0003229318190952418,\n",
       "  0.0003190888651092623,\n",
       "  0.0003172229672027579,\n",
       "  0.00031739651240059175,\n",
       "  0.0003181112689263601,\n",
       "  0.00031905812149354325,\n",
       "  0.00031759629146782336,\n",
       "  0.00031668445242555917,\n",
       "  0.00031690001304485594,\n",
       "  0.0003163019822817727,\n",
       "  0.00031945855045234094,\n",
       "  0.0003189550110686536,\n",
       "  0.000316686581586341,\n",
       "  0.00031807646384415285,\n",
       "  0.0003158634879380568,\n",
       "  0.0003185966638233771,\n",
       "  0.00031818867959103683,\n",
       "  0.0003182917657627345,\n",
       "  0.000317941399434521,\n",
       "  0.00031795412926132686,\n",
       "  0.00032075440483620495,\n",
       "  0.0003178728089820921,\n",
       "  0.0003178816607234896,\n",
       "  0.00031605399595314846,\n",
       "  0.0003175105717942912,\n",
       "  0.0003168746328878182,\n",
       "  0.0003161857541137444,\n",
       "  0.00031617257479444596,\n",
       "  0.0003185741390754831,\n",
       "  0.00031680398522574387,\n",
       "  0.0003160235470137128,\n",
       "  0.0003165600375925553,\n",
       "  0.0003176382388456086,\n",
       "  0.00031670052932760746,\n",
       "  0.00031635286925140664,\n",
       "  0.00031627976406727813,\n",
       "  0.0003165279993953408,\n",
       "  0.00031615186809277575,\n",
       "  0.0003159854486175054,\n",
       "  0.0003161558340286353,\n",
       "  0.00031615142183404206,\n",
       "  0.00031653953812093715,\n",
       "  0.0003166073919500276,\n",
       "  0.00031693983166607923,\n",
       "  0.00031607089988904555,\n",
       "  0.0003164584249370145,\n",
       "  0.0003167942640074115,\n",
       "  0.00031665791512178084,\n",
       "  0.0003161789552703138,\n",
       "  0.00031654011480794806,\n",
       "  0.0003164769291793669,\n",
       "  0.00031608989337217753,\n",
       "  0.00031621471794938705,\n",
       "  0.0003169697850319574,\n",
       "  0.00031623783892158555,\n",
       "  0.00031621799900205115,\n",
       "  0.0003161959640338965,\n",
       "  0.0003160385939635388,\n",
       "  0.0003161292833856014,\n",
       "  0.0003161287243495247,\n",
       "  0.00031608027549939125,\n",
       "  0.00031607725638120127,\n",
       "  0.00031600924382844286,\n",
       "  0.000316017660494523,\n",
       "  0.00031615084097009257,\n",
       "  0.00031630803021942956,\n",
       "  0.0003162505411267005,\n",
       "  0.00031639210027168264,\n",
       "  0.00031617917031528324,\n",
       "  0.00031644790349808454,\n",
       "  0.0003161541701249209,\n",
       "  0.0003160738775073292,\n",
       "  0.0003161037554188321,\n",
       "  0.00031609965231769755,\n",
       "  0.00031608696223917867,\n",
       "  0.00031608787564133917,\n",
       "  0.0003160822812383736,\n",
       "  0.00031610639942098514,\n",
       "  0.0003160933447361458,\n",
       "  0.00031616762741274823,\n",
       "  0.00031606594603982995,\n",
       "  0.0003160986412290691,\n",
       "  0.0003160367775341944,\n",
       "  0.00031606791162630543,\n",
       "  0.0003160630984456037])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = DataBatch(\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    test_loader = test_loader,\n",
    ")\n",
    "\n",
    "regressor = TransformerNet(in_dim = in_dim, hidden=40, window=window, horizon=horizon, nhead=4, num_layers=2, dropout=0.1)\n",
    "opt = torch.optim.Adam(regressor.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=10)\n",
    "trainer = Trainer(model=regressor, loss_fn=\"mse\", opt=opt, scheduler=scheduler, device=\"cuda\")\n",
    "trainer.fit(databatch=databatch, epochs=epochs, verbose=False, filename=\"0730_2330/2330\", patience=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec205adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambastock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
